{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf  \n",
    "import tensorflow_federated as tff\n",
    "from PIL import Image\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file 39_1_20170116174525125.jpg.chip.jpg: not enough values to unpack (expected 4, got 3)\n",
      "Skipping file 61_1_20170109142408075.jpg.chip.jpg: not enough values to unpack (expected 4, got 3)\n",
      "Skipping file 61_1_20170109150557335.jpg.chip.jpg: not enough values to unpack (expected 4, got 3)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "dataset_path = 'UTKFace' \n",
    "\n",
    "# Initialize lists to hold images and labels\n",
    "images = []\n",
    "ages = []\n",
    "genders = []\n",
    "races = []\n",
    "\n",
    "# Load images and extract labels from filenames\n",
    "for img_name in os.listdir(dataset_path):\n",
    "    if img_name.endswith('.jpg'):\n",
    "        parts = img_name.split('_')\n",
    "        if len(parts) >= 4:\n",
    "            try:\n",
    "                age, gender, race = parts[:3]\n",
    "                ages.append(int(age))\n",
    "                genders.append(int(gender))\n",
    "                races.append(int(race))\n",
    "\n",
    "                img_path = os.path.join(dataset_path, img_name)\n",
    "                img = Image.open(img_path).resize((32, 32))\n",
    "                img = np.array(img)\n",
    "                images.append(img)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping file {img_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping file {img_name}: not enough values to unpack (expected 4, got {len(parts)})\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "ages = np.array(ages)\n",
    "genders = np.array(genders)\n",
    "races = np.array(races)\n",
    "\n",
    "# Encode race labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_data_race_encoded = label_encoder.fit_transform(races)\n",
    "\n",
    "# Normalize the images\n",
    "images = images.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to facilitate balancing\n",
    "data = {\n",
    "    'image': list(images),\n",
    "    'age': ages,\n",
    "    'gender': genders,\n",
    "    'race': y_data_race_encoded\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Balance the dataset by oversampling the minority classes\n",
    "df_majority = df[df.race == df.race.mode()[0]]\n",
    "df_minority = df[df.race != df.race.mode()[0]]\n",
    "\n",
    "# Oversample the minority classes\n",
    "df_minority_oversampled = resample(df_minority,\n",
    "                                   replace=True,  # Sample with replacement\n",
    "                                   n_samples=len(df_majority),  # Match number of majority class\n",
    "                                   random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine majority class with oversampled minority classes\n",
    "data_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Extract balanced images and labels\n",
    "images_balanced = np.array(list(data_balanced['image']))\n",
    "ages_balanced = np.array(data_balanced['age'])\n",
    "genders_balanced = np.array(data_balanced['gender'])\n",
    "races_balanced = np.array(data_balanced['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(images_balanced, races_balanced, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated data for training\n",
    "def create_federated_data(data, labels, num_clients=10):\n",
    "    data_size = len(data)\n",
    "    client_data = []\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * data_size // num_clients\n",
    "        end_idx = (i + 1) * data_size // num_clients\n",
    "        client_data.append((data[start_idx:end_idx], labels[start_idx:end_idx]))\n",
    "    return client_data\n",
    "\n",
    "federated_train_data = create_federated_data(x_train, y_train)\n",
    "\n",
    "def preprocess(dataset):\n",
    "    def batch_format_fn(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        return (tf.reshape(image, [32, 32, 3]), tf.reshape(label, [1]))\n",
    "    return dataset.map(batch_format_fn).batch(20)\n",
    "\n",
    "federated_train_dataset = [preprocess(tf.data.Dataset.from_tensor_slices((client[0], client[1]))) for client in federated_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.49175143), ('loss', 1.3505528)]))])\n",
      "Round 2, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5106673), ('loss', 1.3023571)]))])\n",
      "Round 3, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5308236), ('loss', 1.2607901)]))])\n",
      "Round 4, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.54949147), ('loss', 1.2184476)]))])\n",
      "Round 5, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5652444), ('loss', 1.1777991)]))])\n",
      "Round 6, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5781444), ('loss', 1.1371813)]))])\n",
      "Round 7, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.592843), ('loss', 1.0959508)]))])\n",
      "Round 8, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.60723144), ('loss', 1.0560889)]))])\n",
      "Round 9, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6244108), ('loss', 1.019992)]))])\n",
      "Round 10, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.63576037), ('loss', 0.9882742)]))])\n",
      "Round 11, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6479782), ('loss', 0.9605304)]))])\n",
      "Round 12, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6577152), ('loss', 0.9365114)]))])\n",
      "Round 13, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6683825), ('loss', 0.91520983)]))])\n",
      "Round 14, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6778715), ('loss', 0.8959468)]))])\n",
      "Round 15, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.68599606), ('loss', 0.8787197)]))])\n",
      "Round 16, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.692384), ('loss', 0.8631372)]))])\n",
      "Round 17, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6990201), ('loss', 0.84921247)]))])\n",
      "Round 18, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7061523), ('loss', 0.835873)]))])\n",
      "Round 19, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.71161), ('loss', 0.82367533)]))])\n",
      "Round 20, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.71483505), ('loss', 0.8122194)]))])\n",
      "Round 21, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.71855617), ('loss', 0.80142516)]))])\n",
      "Round 22, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7227115), ('loss', 0.79137236)]))])\n",
      "Round 23, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7268048), ('loss', 0.7816133)]))])\n",
      "Round 24, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7294716), ('loss', 0.7722112)]))])\n",
      "Round 25, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7343711), ('loss', 0.7633857)]))])\n",
      "Round 26, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7369759), ('loss', 0.75490546)]))])\n",
      "Round 27, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7395187), ('loss', 0.74672943)]))])\n",
      "Round 28, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7416894), ('loss', 0.73868316)]))])\n",
      "Round 29, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.74441826), ('loss', 0.73109025)]))])\n",
      "Round 30, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.74758124), ('loss', 0.7237004)]))])\n",
      "Round 31, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7503101), ('loss', 0.7165738)]))])\n",
      "Round 32, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7522947), ('loss', 0.7095345)]))])\n",
      "Round 33, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7548995), ('loss', 0.70298374)]))])\n",
      "Round 34, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7573803), ('loss', 0.69640213)]))])\n",
      "Round 35, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.76041925), ('loss', 0.69022596)]))])\n",
      "Round 36, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7625279), ('loss', 0.6839215)]))])\n",
      "Round 37, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7654428), ('loss', 0.6779644)]))])\n",
      "Round 38, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7676755), ('loss', 0.6721987)]))])\n",
      "Round 39, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.77015626), ('loss', 0.6665695)]))])\n",
      "Round 40, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.772389), ('loss', 0.66095)]))])\n",
      "Round 41, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.77424955), ('loss', 0.65544784)]))])\n",
      "Round 42, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7749938), ('loss', 0.6501265)]))])\n",
      "Round 43, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7772885), ('loss', 0.6450165)]))])\n",
      "Round 44, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7791491), ('loss', 0.6399385)]))])\n",
      "Round 45, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7803895), ('loss', 0.6347967)]))])\n",
      "Round 46, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7824981), ('loss', 0.6299882)]))])\n",
      "Round 47, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7834904), ('loss', 0.62539923)]))])\n",
      "Round 48, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7849169), ('loss', 0.6207026)]))])\n",
      "Round 49, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.78733563), ('loss', 0.6162301)]))])\n",
      "Round 50, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.78863806), ('loss', 0.61170256)]))])\n",
      "Round 51, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7903126), ('loss', 0.60730726)]))])\n",
      "Round 52, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.79248327), ('loss', 0.60308415)]))])\n",
      "Round 53, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.79310346), ('loss', 0.5985464)]))])\n",
      "Round 54, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.79484), ('loss', 0.5940797)]))])\n",
      "Round 55, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7973828), ('loss', 0.58996624)]))])\n",
      "Round 56, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.7989953), ('loss', 0.5858256)]))])\n",
      "Round 57, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8000496), ('loss', 0.58178216)]))])\n",
      "Round 58, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.80153805), ('loss', 0.577714)]))])\n",
      "Round 59, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8029645), ('loss', 0.57377845)]))])\n",
      "Round 60, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8042669), ('loss', 0.5699096)]))])\n",
      "Round 61, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.80656165), ('loss', 0.5656307)]))])\n",
      "Round 62, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8074299), ('loss', 0.56175226)]))])\n",
      "Round 63, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.80885637), ('loss', 0.5580204)]))])\n",
      "Round 64, Metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.8096006), ('loss', 0.55393344)]))])\n",
      "Test Metrics: OrderedDict([('sparse_categorical_accuracy', 0.80109125), ('loss', 0.5945425)])\n"
     ]
    }
   ],
   "source": [
    "# Define model function\n",
    "def model_fn():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # Adjust number of classes as needed\n",
    "    ])\n",
    "    return tff.learning.from_keras_model(\n",
    "        model,\n",
    "       input_spec=(tf.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32), \n",
    "            tf.TensorSpec(shape=[None, 1], dtype=tf.int64)),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "# Define client optimizer function\n",
    "def client_optimizer_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "# Build federated averaging process\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=model_fn,\n",
    "    client_optimizer_fn=client_optimizer_fn\n",
    ")\n",
    "\n",
    "# Initialize the process\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "# Train the model for a few rounds\n",
    "for round_num in range(1, 65):\n",
    "    state, metrics = iterative_process.next(state, federated_train_dataset)\n",
    "    print(f'Round {round_num}, Metrics={metrics}')\n",
    "\n",
    "# Prepare test data for federated evaluation\n",
    "federated_test_data = [preprocess(tf.data.Dataset.from_tensor_slices((x_test, y_test)))]\n",
    "\n",
    "# Build federated evaluation process\n",
    "federated_eval = tff.learning.build_federated_evaluation(model_fn)\n",
    "\n",
    "# Evaluate the model on the federated test data\n",
    "test_metrics = federated_eval(state.model, federated_test_data)\n",
    "print(f'Test Metrics: {test_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the trained model weights from the federated learning state\n",
    "model_weights = state.model.trainable\n",
    "\n",
    "# Define the Keras model architecture consistently\n",
    "def create_keras_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # Adjust number of classes as needed\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the Keras model\n",
    "keras_model = create_keras_model()\n",
    "keras_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "keras_model.set_weights(model_weights)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "y_pred = keras_model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: 0\n",
      "Accuracy: 0.9081, Precision: 0.2000, Recall: 0.1816, F1 Score: 0.1904\n",
      "\n",
      "Race: 1\n",
      "Accuracy: 0.8246, Precision: 0.2000, Recall: 0.1649, F1 Score: 0.1808\n",
      "\n",
      "Race: 2\n",
      "Accuracy: 0.7365, Precision: 0.2000, Recall: 0.1473, F1 Score: 0.1697\n",
      "\n",
      "Race: 3\n",
      "Accuracy: 0.7595, Precision: 0.2000, Recall: 0.1519, F1 Score: 0.1727\n",
      "\n",
      "Race: 4\n",
      "Accuracy: 0.1331, Precision: 0.2000, Recall: 0.0266, F1 Score: 0.0470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate metrics for each race\n",
    "for race in np.unique(y_test):\n",
    "    indices = [i for i, r in enumerate(y_test) if r == race]\n",
    "    y_true_race = y_test[indices]\n",
    "    y_pred_race = y_pred[indices]\n",
    "    \n",
    "    accuracy, precision, recall, f1 = evaluate_performance(y_true_race, y_pred_race)\n",
    "    print(f'Race: {race}')\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResAI_EAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
